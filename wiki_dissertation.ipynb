{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe057beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import wikipediaapi  # For accessing Wikipedia\n",
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "import pywikibot\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats \n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98787f07",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa44787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def get_articles(category, level=0, max_level=2):\n",
    "    #seen_categories = set()\n",
    "    #seen_articles = set()\n",
    "    excluded_keywords = [\"violence\", \"list\",\"eunuchs\",\"stubs\",\"deaths from\",\"asexual men\"]  # Keywords to exclude\n",
    "    articles_list = []\n",
    "    cat_members = category.categorymembers.values()\n",
    "\n",
    "    # Create a list of pages and split it into chunks of 200\n",
    "    pages = list(cat_members)\n",
    "    page_chunks = np.array_split(pages, len(pages)//200 + 1)\n",
    "\n",
    "    for chunk in page_chunks:\n",
    "        for c in chunk:\n",
    "            if c.namespace == wikipediaapi.Namespace.CATEGORY and level < max_level and c.title not in seen_categories:\n",
    "                seen_categories.add(c.title)\n",
    "                # Exclude categories with specific keywords\n",
    "                if not any(keyword in c.title.lower() for keyword in excluded_keywords):\n",
    "                    # Check if the category is 'Male genital disorders'\n",
    "                    if c.title in ['Category:Male genital disorders','Category:Male infertility']:\n",
    "                        articles_list.extend(get_articles(c, level=level+1, max_level=max_level+2))  # Increase max_level for this category\n",
    "                    else:\n",
    "                        articles_list.extend(get_articles(c, level=level+1, max_level=max_level))\n",
    "            elif c.namespace == wikipediaapi.Namespace.MAIN and c.title not in seen_articles:\n",
    "                seen_articles.add(c.title)\n",
    "                # Exclude pages with specific keywords in title\n",
    "                if not any(keyword in c.title.lower() for keyword in excluded_keywords):\n",
    "                    articles_list.append(c)\n",
    "    return articles_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = wiki_wiki.page(\"Category:Women's_health\")\n",
    "seen_categories = set()\n",
    "seen_articles = set()\n",
    "women_health_articles = get_articles(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f19bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = wiki_wiki.page(\"Category:Men's_health\")\n",
    "seen_categories = set()\n",
    "seen_articles = set()\n",
    "men_health_articles = get_articles(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CODE\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import pywikibot\n",
    "import urllib.parse\n",
    "from ratelimiter import RateLimiter\n",
    "from datetime import datetime, timedelta\n",
    "from pywikibot.exceptions import PageRelatedError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define the retry limit and delay\n",
    "retry_limit = 3\n",
    "retry_delay = 5  # in seconds\n",
    "\n",
    "# Define the number of threads to use for concurrent requests\n",
    "num_threads = 5\n",
    "\n",
    "HEADERS = {'User-Agent': 'MyResearchProject/2.0 (mm22ajk@leeds.ac.uk) MyResearchProjectDataCollection2'}\n",
    "SITE = pywikibot.Site('en', 'wikipedia')\n",
    "\n",
    "# Set the rate limit values\n",
    "#rate_limit = 40  # Number of requests\n",
    "#rate_period = 60  # Time period in seconds\n",
    "\n",
    "# Configure the rate limiter\n",
    "#rate_limiter = RateLimiter(max_calls=rate_limit, period=rate_period)\n",
    "\n",
    "#@rate_limiter\n",
    "def get_pageviews(title):\n",
    "    try:\n",
    "        title = urllib.parse.quote(title.replace(' ', '_'),safe=' ')\n",
    "        url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/user/{title}/monthly/2022050100/2023053100\"\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()  # raise exception if the request failed\n",
    "        data = response.json()\n",
    "        total_yearly_views = sum(item[\"views\"] for item in data[\"items\"])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch pageviews for {title}: {e}\")\n",
    "        total_yearly_views = np.nan\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error parsing JSON response for {title}: {e}\")\n",
    "        total_yearly_views = np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {title}: {e}\")\n",
    "        total_yearly_views = np.nan\n",
    "\n",
    "    return total_yearly_views\n",
    "\n",
    "def get_link_counts(title):\n",
    "    results = {}\n",
    "    page = pywikibot.Page(SITE, title)\n",
    "    results[\"external_links\"] = len(list(page.extlinks()))\n",
    "    results[\"internal_links\"] = len(list(page.linkedPages()))\n",
    "    return results\n",
    "\n",
    "def get_edit_history(title):\n",
    "    try:\n",
    "        page = pywikibot.Page(SITE, title)\n",
    "\n",
    "        # Get the number of revisions\n",
    "        num_revisions = page.revision_count()\n",
    "\n",
    "        # Get the main author (first contributor) and date of creation\n",
    "        main_author = page.oldest_revision.user\n",
    "        date_of_creation = page.oldest_revision.timestamp.date()\n",
    "\n",
    "        # Get the number of recent edits in the last 30 days\n",
    "        thirty_days_ago = datetime.now() - timedelta(days=30)\n",
    "        recent_edits = [\n",
    "            rev for rev in page.revisions(content=False) if rev.timestamp > thirty_days_ago\n",
    "        ]\n",
    "        num_recent_edits = len(recent_edits)\n",
    "\n",
    "        # Get the date of the latest edit\n",
    "        latest_revision = next(page.revisions(total=1, content=False))\n",
    "        date_of_latest_edit = latest_revision.timestamp.date()\n",
    "\n",
    "        # Return the collected information\n",
    "        edithistory = {\n",
    "            'num_revisions': num_revisions,\n",
    "            'main_author': main_author,\n",
    "            'date_of_creation': date_of_creation,\n",
    "            'num_recent_edits': num_recent_edits,\n",
    "            'date_of_latest_edit': date_of_latest_edit\n",
    "        }\n",
    "\n",
    "        return edithistory\n",
    "\n",
    "    except PageRelatedError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    except requests.exceptions.ReadTimeout as e:\n",
    "        print(\"Timeout occurred. Retrying after 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        return get_edit_history(title)\n",
    "\n",
    "\n",
    "# Configure the rate limiter\n",
    "#rate_limiter_bot = RateLimit(rate=rate_limit, period=rate_period)\n",
    "\n",
    "# Set the rate limiter for all API requests\n",
    "#pywikibot.config.rate_limit = rate_limiter\n",
    "\n",
    "# Set the rate limiter for the get_edithistory function\n",
    "#get_edit_history = rate_limiter(get_edit_history)\n",
    "\n",
    "\n",
    "def get_article_details(article):\n",
    "    pageid = article.pageid\n",
    "    title = article.title\n",
    "    content = article.text\n",
    "    page_view_year = get_pageviews(title)\n",
    "    link_counts = get_link_counts(title)\n",
    "    edit_history = get_edit_history(title)\n",
    "    details = {\n",
    "        'pageid': pageid,\n",
    "        'title': title,\n",
    "        'author' : edit_history['main_author'],\n",
    "        'date_of_creation' : edit_history['date_of_creation'],\n",
    "        'num_revs' : edit_history['num_revisions'],\n",
    "        'num_recent_edits' : edit_history['num_recent_edits'],\n",
    "        'date_of_latest_edit' : edit_history['date_of_latest_edit'],\n",
    "        'page_views_yr': page_view_year,\n",
    "        'internal_links': link_counts[\"internal_links\"],\n",
    "        'external_links': link_counts[\"external_links\"],\n",
    "        'word_count': len(content.split()),\n",
    "        'content': content}\n",
    "    return details\n",
    "\n",
    "def collect_data(articles):\n",
    "    data = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for article in articles:\n",
    "            retry_count = 0\n",
    "            success = False\n",
    "            \n",
    "            while retry_count < retry_limit and not success:\n",
    "                try:\n",
    "                    article_details = get_article_details(article)\n",
    "                    data.append(article_details)\n",
    "                    success = True\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error in API request for {article.title}: {e}\")\n",
    "                    retry_count += 1\n",
    "                    time.sleep(retry_delay)\n",
    "                except Exception as e:\n",
    "                    print(f\"An unexpected error occurred for {article.title}: {e}\")\n",
    "                    success = True  # Skip the current article\n",
    "                    \n",
    "    return data\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 500\n",
    "\n",
    "# specify womens or mens article here\n",
    "# Split the articles into chunks\n",
    "article_chunks = [women_health_articles[i:i + chunk_size] for i in range(0, len(women_health_articles), chunk_size)]\n",
    "\n",
    "# Create an empty DataFrame to store the data\n",
    "final_women_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the chunks\n",
    "for chunk in article_chunks:\n",
    "    print('Starts processing a chunk')\n",
    "    # Start the timer for each chunk\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process the articles in the chunk\n",
    "    data = collect_data(chunk)\n",
    "\n",
    "    # End the timer for each chunk\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the total time for the chunk\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # Convert total_time to minutes and seconds\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "    print('Total execution time for chunk: {} minutes {} seconds'.format(minutes, seconds))\n",
    "    print('Waiting for 5 mins')\n",
    "    print('_________________________________________________________________________')\n",
    "    # Wait for 5 minutes before processing the next chunk\n",
    "    time.sleep(300)  # 5 minutes = 5 * 60 seconds\n",
    "    \n",
    "    # Append the data for the chunk to the final DataFrame\n",
    "    final_women_df = final_women_df.append(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_quality(title):\n",
    "    S = requests.Session()\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"categories\",\n",
    "        \"titles\": f\"Talk:{title}\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        R.raise_for_status()  # Raises stored HTTPError, if one occurred.\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error with request for {title}: {e}\")\n",
    "        return \"Request Error\"\n",
    "\n",
    "    try:\n",
    "        DATA = R.json()\n",
    "        pages = DATA[\"query\"][\"pages\"]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error with response data for {title}: {e}\")\n",
    "        return \"Data Error\"\n",
    "\n",
    "    quality_classes = ['FA-Class', 'GA-Class', 'A-Class', 'B-Class', 'C-Class', 'Start-Class', 'Stub-Class']\n",
    "\n",
    "    quality_assessments = []\n",
    "    for k, v in pages.items():\n",
    "        if 'missing' in v:  # Page does not exist\n",
    "            print(f\"Talk page does not exist for {title}\")\n",
    "            return np.nan\n",
    "        for cat in v.get(\"categories\", []):  # get method with default value to avoid KeyError\n",
    "            for quality_class in quality_classes:\n",
    "                if quality_class in cat[\"title\"]:\n",
    "                    quality_assessments.append(quality_class)\n",
    "                    break\n",
    "\n",
    "    if quality_assessments:\n",
    "        quality = next(quality_class for quality_class in quality_classes if quality_class in quality_assessments)\n",
    "    else:\n",
    "        quality = np.nan\n",
    "\n",
    "    return quality\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27585dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "SITE = pywikibot.Site('en', 'wikipedia')\n",
    "def get_unique_editors(article_title):\n",
    "    page = pywikibot.Page(SITE, article_title)\n",
    "    revisions = page.revisions()\n",
    "    unique_editors = set()\n",
    "    for revision in revisions:\n",
    "        anon = revision.anon\n",
    "        editor = revision.user\n",
    "        if editor and not anon:\n",
    "            unique_editors.add(str(editor))\n",
    "    return len(unique_editors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_df['unique_reg_editors'] = women_df['title'].apply(get_unique_editors)\n",
    "men_df['unique_reg_editors'] = men_df['title'].apply(get_unique_editors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdec9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "women_df = pd.read_csv('final_women_health_articles_13_aug.csv')\n",
    "men_df = pd.read_csv('final_men_health_articles_13_aug.csv')\n",
    "\n",
    "# Add a 'gender' column to each dataset\n",
    "women_df['article_type'] = 'Women'\n",
    "men_df['article_type'] = 'Men'\n",
    "\n",
    "# Combine the two datasets\n",
    "df_combined = pd.concat([women_df, men_df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192c6d1",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fd768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date_of_creation' to datetime format\n",
    "men_df['date_of_creation'] = pd.to_datetime(men_df['date_of_creation'])\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "men_df['age'] = (current_date - men_df['date_of_creation']).astype('<m8[Y]')\n",
    "men_df['age'] = men_df['age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c43783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_content(content):\n",
    "    # Remove everything after '\\n See also \\n', '\\n References \\n', or '\\n External links \\n'\n",
    "    content = re.split('\\nSee also\\n|\\nReferences\\n|\\n== References ==|\\nExternal links\\n', content)[0]\n",
    "    \n",
    "\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "test_m_df['content_preprocessed'] = test_m_df['content'].apply(preprocess_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(content):\n",
    "    return len(content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_content(content):\n",
    "    # Remove everything after '\\n See also \\n', '\\n References \\n', or '\\n External links \\n'\n",
    "    ##soup = BeautifulSoup(content, 'html.parser')\n",
    "    #text = soup.get_text()\n",
    "    #text = text.replace('\\n', ' ')\n",
    "    \n",
    "    doc = nlp(content)\n",
    "\n",
    "    # Remove named entities, phrases, and concepts\n",
    "    cleaned_text = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ not in [\"PERSON\", \"ORG\"]:\n",
    "            if token.is_alpha and token.text.lower() not in stop_words:\n",
    "                cleaned_text.append(lemmatizer.lemmatize(token.text.lower()))\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    clean_text = ' '.join(cleaned_text)\n",
    "    return clean_text\n",
    "\n",
    "men_df['content_after_nlp'] = men_df['content_preprocessed'].apply(preprocess_content)\n",
    "women_df['content_after_nlp'] = women_df['content_preprocessed'].apply(preprocess_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7761e",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed218da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_combined, x= \"Age\", y=\"Page Views per Year\", marginal_x=\"histogram\", marginal_y=\"rug\", color=\"article_type\",\n",
    "                 color_continuous_scale=[\"green\", \"red\"])\n",
    "fig.update_layout(title_text=\"Age vs Page Views\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_combined, x=\"Page Views per Year\", y=\"Number of Revisions\",  color=\"article_type\",facet_col=\"article_type\",marginal_x=\"histogram\",\n",
    "                 trendline=\"ols\")\n",
    "fig.update_layout(title_text = \"Page Views vs Number of Revisions: For men's and women's articles\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(data=df_combined, x='Age', hue='article_type', bins=20, palette=['lightcoral', 'blue'], element=\"step\")\n",
    "plt.title('Histogram of Age')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not good, if u wanna keep this, find similar plots\n",
    "df_combined = df_combined.sort_values(by='article_type', ascending=False)\n",
    "\n",
    "fig = px.histogram(df_combined, x = 'Age', nbins = 20, text_auto=True,color='article_type',marginal=\"box\",\n",
    "                   color_discrete_sequence=['lightcoral','blue'], title='Histogram of Age')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac39fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a categorical type for the quality levels, in increasing order of quality\n",
    "quality_type = pd.CategoricalDtype(categories=['FA-Class', 'A-Class', 'GA-Class', 'B-Class', 'C-Class', 'Start-Class', 'Stub-Class', 'Unassessed'], ordered=True)\n",
    "\n",
    "# Function to get quality distribution for a dataframe\n",
    "def get_quality_distribution(df, df_name):\n",
    "    quality_distribution = df['quality'].value_counts(normalize=True) * 100\n",
    "    return pd.DataFrame(quality_distribution).reset_index().rename(columns={'index': 'Quality', 'quality': f'{df_name} Articles (%)'})\n",
    "\n",
    "# Get quality distribution for women's and men's health articles\n",
    "quality_distribution_women = get_quality_distribution(women_df, \"Women's Health\")\n",
    "quality_distribution_men = get_quality_distribution(men_df, \"Men's Health\")\n",
    "\n",
    "# Merge both dataframes for easier comparison\n",
    "quality_distribution = pd.merge(quality_distribution_women, quality_distribution_men, on='Quality', how='outer')\n",
    "\n",
    "# Convert the 'Quality' column to the categorical type\n",
    "quality_distribution['Quality'] = quality_distribution['Quality'].astype(quality_type)\n",
    "\n",
    "# Sort the dataframe by the 'Quality' column\n",
    "quality_distribution_sorted = quality_distribution.sort_values(by='Quality')\n",
    "\n",
    "quality_distribution_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation matrix is displayed here. It gives an idea of how various features are related to one another.\n",
    "num_variables = ['Number of Revisions', 'Page Views per Year','Number of Recent Edits', 'Word Count','Internal Links', 'External Links','Age']\n",
    "\n",
    "men_df_corr = df_combined[df_combined['article_type'] == 'Men']\n",
    "women_df_corr = df_combined[df_combined['article_type'] == 'Women']\n",
    "\n",
    "# Correlation matrix for men's articles\n",
    "cormatrix_men = men_df_corr[num_variables].corr()\n",
    "\n",
    "# Correlation matrix for women's articles\n",
    "cormatrix_women = women_df_corr[num_variables].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a427a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation matrix is visualised for quick inferences.\n",
    "plt.figure(figsize=(18, 6))\n",
    "heatmap = sns.heatmap(cormatrix_men, vmin=-1, annot=True, cmap='RdBu',annot_kws={\"size\": 14})\n",
    "heatmap.set_title(\"Men's Correlation Heatmap\", fontdict={'fontsize':20}, pad=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b983f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation matrix is visualised for quick inferences.\n",
    "plt.figure(figsize=(18, 6))\n",
    "heatmap = sns.heatmap(cormatrix_women, vmin=-1, annot=True, cmap='RdBu', annot_kws={\"size\": 14})\n",
    "heatmap.set_title(\"Women's Correlation Heatmap\", fontdict={'fontsize':20}, pad=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots to check for potential outliers\n",
    "num_variables = ['word_count','age','num_revs','num_recent_edits','page_views_yr','unique_reg_editors','internal_links', 'external_links']\n",
    "\n",
    "#Creating subplot of each column with its own scale\n",
    "red_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white') #the outliers are marked with a red circle\n",
    "fig, axs = plt.subplots(1, len(num_variables), figsize=(20,10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.boxplot(women_df[num_variables[i]], flierprops=red_circle)\n",
    "    #ax.set_title(num_variables[i], fontsize=20, fontweight='bold', rotation=10)\n",
    "    ax.set_xlabel(num_variables[i], fontsize=15)\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    \n",
    "plt.tight_layout()\n",
    "fig.suptitle('Women\\'s Health Boxplot', fontsize=25, fontweight='bold', y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154317f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots to check for potential outliers\n",
    "num_variables = ['word_count','age','num_revs','num_recent_edits','page_views_yr','unique_reg_editors','internal_links', 'external_links']\n",
    "\n",
    "# Create a mapping dictionary for x-axis labels\n",
    "attributes_map = {\n",
    "    'word_count': 'Word Count',\n",
    "    'age': 'Age of Article',\n",
    "    'num_revs': 'Number of Revisions',\n",
    "    'num_recent_edits': 'Number of Recent Edits',\n",
    "    'page_views_yr': 'Page Views/Year',\n",
    "    'unique_reg_editors': 'Unique Registered Editors',\n",
    "    'internal_links': 'Internal Links',\n",
    "    'external_links': 'External Links'\n",
    "}\n",
    "\n",
    "# Creating subplot of each column with its own scale\n",
    "red_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white') #the outliers are marked with a red circle\n",
    "fig, axs = plt.subplots(1, len(num_variables), figsize=(20,10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.boxplot(men_df[num_variables[i]], flierprops=red_circle)\n",
    "    ax.set_xlabel(attributes_map[num_variables[i]], fontsize=15)\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    \n",
    "plt.tight_layout()\n",
    "fig.suptitle('Boxplots for the Numerical variables of Men\\'s Health Dataset', fontsize=25, fontweight='bold', y=1.05)\n",
    "plt.savefig('/Users/aryajeshkumar/Desktop/Study/dissertation/writing/images/men_boxplot.png', dpi = 450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea0b25",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def perform_ttests(dataset1, dataset2, alpha=0.05):\n",
    "    results_list = []\n",
    "    for column in dataset1.columns:\n",
    "        if column == 'pageid':\n",
    "            continue\n",
    "        if dataset1[column].dtype in ['int64', 'float64'] and column in dataset2.columns:\n",
    "            group1 = dataset1[column]\n",
    "            group2 = dataset2[column]\n",
    "            \n",
    "            # Perform Welch's t-test\n",
    "            t_statistic, p_value = ttest_ind(group1, group2, equal_var=False)\n",
    "            \n",
    "            # Calculate variances\n",
    "            var_group1 = np.var(group1, ddof=1)\n",
    "            var_group2 = np.var(group2, ddof=1)\n",
    "\n",
    "            # Calculate sample sizes\n",
    "            n1 = len(group1)\n",
    "            n2 = len(group2)\n",
    "            \n",
    "            # Calculate the degrees of freedom\n",
    "            df = ((var_group1 / n1 + var_group2 / n2)**2) / \\\n",
    "                    ((var_group1 / n1)**2 / (n1 - 1) + (var_group2 / n2)**2 / (n2 - 1))\n",
    "            \n",
    "            # Determine the t critical value\n",
    "            t_critical = t.ppf(1 - alpha / 2, df=df)\n",
    "            \n",
    "            # Calculate the standard error\n",
    "            se = np.sqrt(var_group1 / n1 + var_group2 / n2)\n",
    "            \n",
    "            # Calculate the margin of error\n",
    "            margin_of_error = t_critical * se\n",
    "\n",
    "            # Confidence interval for the difference between means\n",
    "            mean_diff = np.mean(group1) - np.mean(group2)\n",
    "            ci_lower, ci_upper = mean_diff - margin_of_error, mean_diff + margin_of_error\n",
    "            \n",
    "            # Check if the result is significant\n",
    "            is_significant = p_value < alpha\n",
    "            \n",
    "            results_list.append({\n",
    "                'Attribute': column,\n",
    "                't_statistic': t_statistic,\n",
    "                'p_value': p_value,\n",
    "                'is_significant': is_significant,\n",
    "                'confidence_interval_lower': ci_lower,\n",
    "                'confidence_interval_upper': ci_upper\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(dataset1, dataset2):\n",
    "    results_list = []\n",
    "    for column in dataset1.columns:\n",
    "        if column == 'pageid':\n",
    "            continue\n",
    "        if dataset1[column].dtype in ['int64', 'float64'] and column in dataset2.columns:\n",
    "            group1 = dataset1[column]\n",
    "            group2 = dataset2[column]\n",
    "            \n",
    "            # Assuming you have these values for men's and women's health articles\n",
    "            mean_men = group1.mean() # mean of men's group\n",
    "            mean_women = group2.mean() # mean of women's group\n",
    "            std_men = group1.std() # standard deviation of men's group\n",
    "            std_women = group2.std() # standard deviation of women's group\n",
    "            n_men = len(group1) # sample size of men's group\n",
    "            n_women = len(group2) # sample size of women's group\n",
    "\n",
    "            # Calculate the pooled standard deviation\n",
    "            pooled_std = sqrt((std_men**2 + std_women**2)/2)\n",
    "\n",
    "            # Calculate Cohen's d\n",
    "            cohens_d = (mean_men - mean_women) / pooled_std\n",
    "            \n",
    "            pooled_sd = sqrt(((n_men - 1)*std_men**2 + (n_women - 1)*std_women**2) / (n_men + n_women - 2))\n",
    "            # Calculate Cohen's d\n",
    "            g_cohens_d = (mean_men - mean_women) / pooled_sd\n",
    "            \n",
    "            correction = (1 - 3 / (4 * (n_men + n_women) - 9))\n",
    "            hedges_g = g_cohens_d * correction\n",
    "            \n",
    "            results_list.append({\n",
    "                'Attribute': column,\n",
    "                'cohens_d': cohens_d,\n",
    "                'hedges_g': hedges_g\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b01bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bar plot\n",
    "fig = px.bar(df, x='cohens_d', y='Attribute', orientation='h', title=\"Effect Sizes (Cohen's d) for Each Attribute\")\n",
    "# Update the color of all bars to 'b'\n",
    "fig.update_traces(marker_color='darkblue')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcac32",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268131a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping quality levels to numerical values\n",
    "quality_mapping = {'Unassessed':0, 'Stub-Class': 1, 'Start-Class': 2, 'C-Class': 3, 'B-Class': 4, 'GA-Class': 5, 'A-Class': 6, 'FA-Class': 7}\n",
    "\n",
    "# Apply the mapping to the quality column\n",
    "women_df['quality'] = women_df['quality'].map(quality_mapping)\n",
    "men_df['quality'] = men_df['quality'].map(quality_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'quality' column with 0\n",
    "women_df['quality'] = women_df['quality'].fillna(0)\n",
    "\n",
    "# Do the same for the men's dataframe\n",
    "men_df['quality'] = men_df['quality'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def feature_analysis(df):\n",
    "    # Initialize an empty DataFrame to store the results\n",
    "    results = pd.DataFrame(columns=['Feature', 'Median', 'Correlation', 'P-value'])\n",
    "\n",
    "    features = ['num_revs', 'num_recent_edits', 'page_views_yr',\n",
    "                'internal_links', 'external_links', 'word_count',\n",
    "                'unique_reg_editors', 'age']\n",
    "    \n",
    "    # Number of tests\n",
    "    num_tests = len(features)\n",
    "    \n",
    "    # Iterate over each feature in the DataFrame\n",
    "    for feature in features:\n",
    "        # Calculate the median of the feature\n",
    "        median = df[feature].median()\n",
    "\n",
    "        # Calculate the Spearman's rank correlation between the feature and quality\n",
    "        correlation, p_value = stats.spearmanr(df[feature], df['quality_mapped'])\n",
    "\n",
    "        # Adjust p-value for Bonferroni correction\n",
    "        adjusted_p_value = min(p_value * num_tests, 1)\n",
    "\n",
    "        # Add the results to the results DataFrame\n",
    "        results = results.append({'Feature': feature, \n",
    "                                  'Median': median, \n",
    "                                  'Correlation': correlation, \n",
    "                                  'P-value': adjusted_p_value}, \n",
    "                                 ignore_index=True)\n",
    "\n",
    "    # Return the results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_results = feature_analysis(women_df)\n",
    "men_results = feature_analysis(men_df)\n",
    "\n",
    "print(\"Women's Data Analysis Results:\")\n",
    "print(women_results)\n",
    "\n",
    "print(\"\\nMen's Data Analysis Results:\")\n",
    "print(men_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6709468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'df_men' and 'df_women' are your DataFrames\n",
    "features = women_results['Feature']\n",
    "custom_labels = ['No of total revisions', 'No of recent revisions\\n(last 30 days)', 'Page views\\n(last 12 months)',\n",
    "                 'No of internal links', 'No of external links', 'Number of words', 'No of unique editors', 'Article age']\n",
    "\n",
    "x = np.arange(len(features))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Add bars for men's data\n",
    "rects1 = ax.bar(x - width/2, men_results['Correlation'], width, label='Men',color='#190A3D')\n",
    "\n",
    "# Add bars for women's data\n",
    "rects2 = ax.bar(x + width/2, women_results['Correlation'], width, label='Women',color='#CAB2FF')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Correlation',fontsize=28)\n",
    "ax.set_title(\"Spearman's Rank Correlation between the Feature & Quality\",fontsize=28)\n",
    "ax.set_xticks(x)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.set_xticklabels(custom_labels, rotation=45, ha=\"right\",fontsize=20 )\n",
    " \n",
    "ax.legend(fontsize=20)\n",
    "fig.tight_layout()\n",
    "# Save the plot as PNG\n",
    "plt.savefig('correlation_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58618d51",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d906398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def get_sentiment(data):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    # Apply sentiment analysis on the clean content\n",
    "    data['sentiment_score'] = data['content_nlp'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "    # Classify sentiment based on the sentiment score\n",
    "    data['sentiment'] = data['sentiment_score'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_df = get_sentiment(women_df)\n",
    "men_df = get_sentiment(men_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the average sentiment scores for men's and women's health articles\n",
    "print(\"Women's Health Articles - Sentiment Analysis:\")\n",
    "print(women_df['sentiment_score'].mean())\n",
    "\n",
    "print(\"\\nMen's Health Articles - Sentiment Analysis:\")\n",
    "print(men_df['sentiment_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for BERT\n",
    "model_name = (\n",
    "    \"textattack/bert-base-uncased-SST-2\"  # This is a model fine-tuned on SST-2 dataset\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # If tokens are too many, truncate them\n",
    "    if len(tokens) > 512:\n",
    "        tokens = tokens[:512]\n",
    "\n",
    "    encoded_texts = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_texts[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded_texts[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    sentiment_idx = logits.argmax().item()  # This will give 0 or 1\n",
    "    confidence = probs[0][\n",
    "        sentiment_idx\n",
    "    ].item()  # Get the confidence of the predicted class\n",
    "\n",
    "    sentiment_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "    # Adjust sentiment to \"Neutral\" if confidence is less than 0.55\n",
    "    if confidence < 0.60:\n",
    "        return (\"Neutral\", confidence)\n",
    "    else:\n",
    "        return (\n",
    "            sentiment_map[sentiment_idx],\n",
    "            confidence,\n",
    "        )  # Returning both sentiment and its confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e75e113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_df_sentiment(df):\n",
    "    sentiments = []\n",
    "    confidences = []\n",
    "    for text in df['content_sa']:\n",
    "        sentiment, confidence = analyze_sentiment(text)\n",
    "        sentiments.append(sentiment)\n",
    "        confidences.append(confidence)\n",
    "    df['bert_label'] = sentiments\n",
    "    df['bert_confidence'] = confidences\n",
    "    return df\n",
    "\n",
    "women_df = analyze_df_sentiment(women_df)\n",
    "men_df = analyze_df_sentiment(men_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create subplots: 1 row, 2 columns\n",
    "fig = make_subplots(rows=2, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}]], subplot_titles=('VADER', 'BERT'))\n",
    "\n",
    "# Define color sequence\n",
    "color_sequence = px.colors.sequential.Plasma_r\n",
    "#ffff33\n",
    "# Define colors for each category\n",
    "color_dict = {'Negative': 'purple', 'Positive': 'lightblue', 'Neutral': 'lightyellow'}\n",
    "#, 'B-Class': '#984ea3', 'C-Class': 'purple', 'Start-Class': 'lightblue', 'Stub-Class': '#a65628', 'Unassessed': 'grey'}\n",
    "\n",
    "# Replace color_sequence with color list mapped from quality categories\n",
    "color_sequence_vader = [color_dict[i] for i in women_df['vader_label'].value_counts().index]\n",
    "color_sequence_bert = [color_dict[i] for i in men_df['bert_label'].value_counts().index]\n",
    "\n",
    "# Continue with your code, replacing color_sequence with color_sequence_women and color_sequence_men in the respective pie chart.\n",
    "\n",
    "\n",
    "# Pie chart for Women's Health Dataset\n",
    "fig.add_trace(go.Pie(labels= men_df['vader_label'].value_counts().index,\n",
    "                     values= men_df['vader_label'].value_counts().values,\n",
    "                     name=\"men_vader\",\n",
    "                     textinfo='label+percent+value',\n",
    "                     marker=dict(colors=color_sequence_vader, line=dict(color='#000000', width=2))),\n",
    "              1, 1)\n",
    "\n",
    "# Pie chart for Men's Health Dataset\n",
    "fig.add_trace(go.Pie(labels= men_df['bert_label'].value_counts().index,\n",
    "                     values= men_df['bert_label'].value_counts().values,\n",
    "                     name=\"Men_bert\",\n",
    "                     textinfo='label+percent+value',\n",
    "                     marker=dict(colors=color_sequence_bert, line=dict(color='#000000', width=2))),\n",
    "              1, 2)\n",
    "# Pie chart for Women's Health Dataset\n",
    "fig.add_trace(go.Pie(labels= women_df['vader_label'].value_counts().index,\n",
    "                     values= women_df['vader_label'].value_counts().values,\n",
    "                     name=\"Women\",\n",
    "                     textinfo='percent+value',\n",
    "                     marker=dict(colors=color_sequence_vader, line=dict(color='#000000', width=2))),\n",
    "              2, 1)\n",
    "\n",
    "# Pie chart for Men's Health Dataset\n",
    "fig.add_trace(go.Pie(labels= women_df['bert_label'].value_counts().index,\n",
    "                     values= women_df['bert_label'].value_counts().values,\n",
    "                     name=\"Men\",\n",
    "                     textinfo='percent+value',\n",
    "                     marker=dict(colors=color_sequence_bert, line=dict(color='#000000', width=2))),\n",
    "              2, 2)\n",
    "\n",
    "# Tune layout and title\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Analysis Results\",\n",
    "    font=dict(size=18),\n",
    "    legend_orientation=\"h\",\n",
    "    legend=dict(x=0.5, y=-0.3, xanchor='center'),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518867e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create subplots: 1 row, 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]], subplot_titles=('VADER', 'BERT'))\n",
    "\n",
    "# Define color sequence\n",
    "color_sequence = px.colors.sequential.Plasma_r\n",
    "#ffff33\n",
    "# Define colors for each category\n",
    "color_dict = {'Negative': 'purple', 'Positive': 'lightblue', 'Neutral': 'lightyellow'}\n",
    "#, 'B-Class': '#984ea3', 'C-Class': 'purple', 'Start-Class': 'lightblue', 'Stub-Class': '#a65628', 'Unassessed': 'grey'}\n",
    "\n",
    "# Replace color_sequence with color list mapped from quality categories\n",
    "color_sequence_vader = [color_dict[i] for i in women_df['vader_label'].value_counts().index]\n",
    "color_sequence_bert = [color_dict[i] for i in men_df['bert_label'].value_counts().index]\n",
    "\n",
    "# Continue with your code, replacing color_sequence with color_sequence_women and color_sequence_men in the respective pie chart.\n",
    "\n",
    "\n",
    "# Pie chart for Women's Health Dataset\n",
    "fig.add_trace(go.Pie(labels= men_df['vader_label'].value_counts().index,\n",
    "                     values= men_df['vader_label'].value_counts().values,\n",
    "                     name=\"men_vader\",\n",
    "                     textinfo='label+percent+value',\n",
    "                     marker=dict(colors=color_sequence_vader, line=dict(color='#000000', width=2)),\n",
    "                    rotation=90),\n",
    "              1, 1)\n",
    "\n",
    "# Pie chart for Men's Health Dataset\n",
    "fig.add_trace(go.Pie(labels= men_df['bert_label'].value_counts().index,\n",
    "                     values= men_df['bert_label'].value_counts().values,\n",
    "                     name=\"Men_bert\",\n",
    "                     textinfo='label+percent+value',\n",
    "                     marker=dict(colors=color_sequence_bert, line=dict(color='#000000', width=2)),\n",
    "                    rotation=90),\n",
    "              1, 2)\n",
    "\n",
    "# Tune layout and title\n",
    "fig.update_layout(\n",
    "    title_text=\"Men's Health Articles\",\n",
    "    #title_y=0.86,\n",
    "    font=dict(size=18),\n",
    "    legend_orientation=\"h\",\n",
    "    legend=dict( x=0.5, y=0,xanchor='center'))\n",
    "\n",
    "fig.show()\n",
    "plt.savefig('/Users/aryajeshkumar/Desktop/Study/dissertation/writing/images/men_sentiment.png', dpi = 450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a dataframe 'df' with two columns 'bert_labels' and 'vader_labels'\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create the confusion matrix\n",
    "matrix = confusion_matrix(men_df['vader_label'], men_df['bert_label'], labels=['Positive', 'Neutral', 'Negative'])\n",
    "\n",
    "# Convert matrix to dataframe for better visualization\n",
    "confusion_df = pd.DataFrame(matrix, index=['Positive', 'Neutral', 'Negative'], columns=['Positive', 'Neutral', 'Negative'])\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.set(font_scale=1.4) # Adjust to your liking\n",
    "sns.heatmap(confusion_df, annot=True, fmt='g', cmap='Blues', annot_kws={\"size\": 18}) # 'annot_kws' to set the font size of the numbers\n",
    "plt.title(\"Men's Confusion Matrix\", fontsize=19)\n",
    "plt.ylabel(\"VADER predictions\", fontsize=17)\n",
    "plt.xlabel(\"BERT predictions\", fontsize=17)\n",
    "plt.xticks(fontsize=15) \n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming both the sentiments are in the same DataFrame\n",
    "disagreement_df = df_combined[df_combined['bert_label'] != df_combined['vader_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c0199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For women's articles\n",
    "disagreement_df_women = disagreement_df[disagreement_df['article_type'] == 'Women']\n",
    "# For men's articles\n",
    "disagreement_df_men = disagreement_df[disagreement_df['article_type'] == 'Men']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Women_bert_neg_vader_pos = disagreement_df_women[(disagreement_df_women['bert_label'] == 'Negative') & (disagreement_df_women['vader_label'] == 'Positive')]\n",
    "Men_bert_neg_vader_pos = disagreement_df_men[(disagreement_df_men['bert_label'] == 'Negative') & (disagreement_df_men['vader_label'] == 'Positive')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_Women_bert_neg_vader_pos = Women_bert_neg_vader_pos.sample(n=2)\n",
    "sample_Men_bert_neg_vader_pos = Men_bert_neg_vader_pos.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all the texts from the disagreement subset\n",
    "women_texts = Women_bert_neg_vader_pos['content_sa'].tolist()\n",
    "\n",
    "# Unigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(women_texts)\n",
    "women_unigrams = pd.DataFrame(X.sum(axis=0), columns=vectorizer.get_feature_names()).T\n",
    "print(\"Unigrams:\")\n",
    "print(women_unigrams.sort_values(0, ascending=False).head(10))\n",
    "\n",
    "# Bigrams\n",
    "vectorizer.set_params(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(women_texts)\n",
    "women_bigrams = pd.DataFrame(X.sum(axis=0), columns=vectorizer.get_feature_names()).T\n",
    "print(\"\\nBigrams:\")\n",
    "print(women_bigrams.sort_values(0, ascending=False).head(10))\n",
    "\n",
    "# Trigrams\n",
    "vectorizer.set_params(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(women_texts)\n",
    "women_trigrams = pd.DataFrame(X.sum(axis=0), columns=vectorizer.get_feature_names()).T\n",
    "print(\"\\nTrigrams:\")\n",
    "print(women_trigrams.sort_values(0, ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all the texts from the disagreement subset\n",
    "men_texts = Men_bert_neg_vader_pos['content_sa'].tolist()\n",
    "\n",
    "# Unigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(men_texts)\n",
    "men_unigrams = pd.DataFrame(X.sum(axis=0), columns=vectorizer.get_feature_names()).T\n",
    "print(\"Unigrams:\")\n",
    "print(men_unigrams.sort_values(0, ascending=False).head(10))\n",
    "\n",
    "# Bigrams\n",
    "vectorizer.set_params(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(men_texts)\n",
    "men_bigrams = pd.DataFrame(X.sum(axis=0), columns=vectorizer.get_feature_names()).T\n",
    "print(\"\\nBigrams:\")\n",
    "print(men_bigrams.sort_values(0, ascending=False).head(10))\n",
    "\n",
    "# Trigrams\n",
    "vectorizer.set_params(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(men_texts)\n",
    "men_trigrams = pd.DataFrame(X.sum(axis=0), columns=vectorizer.get_feature_names()).T\n",
    "print(\"\\nTrigrams:\")\n",
    "print(men_trigrams.sort_values(0, ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a dataframe 'df' with columns 'bert_confidence' and 'vader_score'\n",
    "\n",
    "# Filter for BERT confidence > 0.9\n",
    "high_confidence_articles = disagreement_df_women[disagreement_df_women['bert_confidence'] > 0.9]\n",
    "\n",
    "# From the high confidence subset, filter for VADER scores close to neutral\n",
    "women_neutral_vader = high_confidence_articles[abs(high_confidence_articles['vader_score']) < 0.05]\n",
    "\n",
    "count_of_such_articles = len(women_neutral_vader)\n",
    "\n",
    "print(f\"There are {count_of_such_articles} articles where BERT is highly confident but VADER's score indicates neutrality.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = stats.ttest_ind(men_df['bert_confidence'], women_df['bert_confidence'], equal_var=False)\n",
    "print(f'test statistic: {t}')\n",
    "print(f'p-value: {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3831059",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = stats.ttest_ind(men_df['vader_score'], women_df['vader_score'], equal_var=False)\n",
    "print(f'test statistic: {t}')\n",
    "print(f'p-value: {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dadb2d",
   "metadata": {},
   "source": [
    "## Quantify Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Point this to your downloaded GloVe file\n",
    "glove_input_file = '/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'w2v-output.txt'\n",
    "\n",
    "# Convert GloVe vectors to Word2Vec format\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Now, you can load the Word2Vec format vectors into Gensim as usual\n",
    "from gensim.models import KeyedVectors\n",
    "#word_vectors = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d134ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "women_sentences = [sentence.split() for article in women_df['content_ba'] for sentence in sent_tokenize(article)]\n",
    "your_women_model = Word2Vec(women_sentences, min_count=1, vector_size=100, workers=4, window=5)\n",
    "\n",
    "men_sentences = [sentence.split() for article in men_df['content_ba'] for sentence in sent_tokenize(article)]\n",
    "your_men_model = Word2Vec(men_sentences, min_count=1, vector_size=100, workers=4, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c92061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "men_sentences = [sentence.split() for article in men_df['content_ba'] for sentence in sent_tokenize(article)]\n",
    "\n",
    "# 1. Initialize a fresh Word2Vec model\n",
    "men_model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 2. Build the vocabulary using your dataset\n",
    "men_model.build_vocab(men_sentences)\n",
    "\n",
    "# 3. Load the pre-trained vectors into the wv attribute\n",
    "men_model.wv.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "# 4. Train the model on your dataset\n",
    "men_model.train(men_sentences, total_examples=men_model.corpus_count, epochs=men_model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac90329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the embeddings\n",
    "original_embeddings_men = your_men_model.wv\n",
    "fine_tuned_embeddings_men = men_model_1.wv\n",
    "\n",
    "original_embeddings_women = your_women_model.wv\n",
    "fine_tuned_embeddings_women = women_model.wv\n",
    "\n",
    "# List of health-related terms you're interested in\n",
    "terms = [\"male\",\"man\",\"health\", \"medicine\", \"doctor\", \"hospital\", \"surgery\", \"therapy\", \"disease\",\"nurse\"]\n",
    "\n",
    "# Extract word vectors for the terms\n",
    "original_vectors_men = np.array([original_embeddings_men[word] for word in terms])\n",
    "fine_tuned_vectors_men = np.array([fine_tuned_embeddings_men[word] for word in terms])\n",
    "\n",
    "original_vectors_women = np.array([original_embeddings_women[word] for word in terms])\n",
    "fine_tuned_vectors_women = np.array([fine_tuned_embeddings_women[word] for word in terms])\n",
    "\n",
    "# Reduce dimensions using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=min(30, len(terms) - 1), n_iter=300)\n",
    "original_2d_men = tsne.fit_transform(original_vectors_men)\n",
    "fine_tuned_2d_men = tsne.fit_transform(fine_tuned_vectors_men)\n",
    "\n",
    "original_2d_women = tsne.fit_transform(original_vectors_women)\n",
    "fine_tuned_2d_women = tsne.fit_transform(fine_tuned_vectors_women)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Visualization function for side-by-side plots\n",
    "def plot_combined_embeddings(original_men, fine_tuned_men, original_women, fine_tuned_women, labels):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Men's embeddings on the left subplot\n",
    "    for i, label in enumerate(labels):\n",
    "        axes[0].scatter(*original_men[i], color='red', label=\"Original\" if i == 0 else \"\")\n",
    "        axes[0].scatter(*fine_tuned_men[i], color='blue', label=\"Fine-tuned\" if i == 0 else \"\")\n",
    "        axes[0].annotate(label, fine_tuned_men[i], textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "        axes[0].annotate(label, original_men[i], textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "        \n",
    "    # Women's embeddings on the right subplot\n",
    "    for i, label in enumerate(labels):\n",
    "        axes[1].scatter(*original_women[i], color='red', label=\"Original\" if i == 0 else \"\")\n",
    "        axes[1].scatter(*fine_tuned_women[i], color='blue', label=\"Fine-tuned\" if i == 0 else \"\")\n",
    "        axes[1].annotate(label, fine_tuned_women[i], textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "        axes[1].annotate(label, original_women[i], textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "        \n",
    "    axes[0].set_title(\"Men's Articles Embedding\")\n",
    "    axes[1].set_title(\"Women's Articles Embedding\")\n",
    "    axes[0].legend(loc=\"upper right\")\n",
    "    axes[1].legend(loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"combined_embed.png\", format='png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot combined embeddings\n",
    "plot_combined_embeddings(original_2d_men, fine_tuned_2d_men, original_2d_women, fine_tuned_2d_women, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1aafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming you have a list of articles\n",
    "articles = pd.concat([women_df['content_ba'], men_df['content_ba']], ignore_index=True)\n",
    "\n",
    "attribute_sets = {\n",
    "        \"B1_X\": ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career'],\n",
    "        \"B1_Y\": ['home', 'parents', 'children', 'family',  'marriage', 'wedding'],\n",
    "        \"B2_X\": ['math',  'geometry', 'calculus', 'computation','addition'],\n",
    "        \"B2_Y\": ['poetry', 'art', 'shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama'],\n",
    "        \"B3_X\": ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment'],\n",
    "        \"B3_Y\": ['poetry', 'art', 'shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama'], # This is repeated but keeping it as provided\n",
    "        \"B4_X\": ['precocious', 'resourceful',  'genius', 'inventive', 'astute', 'adaptable', 'reflective',  'intuitive',  'judicious', 'analytical', 'apt', 'venerable', 'imaginative', 'shrewd', 'wise', 'smart', 'clever', 'brilliant', 'logical', 'intelligent'],\n",
    "        \"B4_Y\": ['alluring', 'voluptuous',  'plump', 'sensual', 'gorgeous', 'slim', 'bald', 'athletic', 'fashionable', 'stout', 'ugly', 'muscular', 'slender', 'feeble', 'handsome', 'healthy', 'attractive', 'fat', 'weak', 'thin', 'pretty', 'beautiful', 'strong'],\n",
    "        \"B5_X\": ['power', 'strong', 'confident', 'dominant', 'potent', 'command', 'assert', 'loud', 'bold', 'succeed', 'triumph', 'leader', 'shout', 'dynamic', 'winner'],\n",
    "        \"B5_Y\": ['weak', 'surrender',  'vulnerable', 'weakness',  'withdraw', 'yield', 'failure', 'shy', 'follow', 'lose', 'fragile', 'afraid'],\n",
    "        \"H1_X\": ['surgeon', 'doctor', 'orthopedic', 'paramedic', 'pharmacist'],\n",
    "        \"H1_Y\": ['nurse', 'midwife', 'pediatrician', 'assistant', 'dietician'],\n",
    "        \"H2_X\": ['muscular', 'athletic', 'stoic', 'strong', 'unemotional'],\n",
    "        \"H2_Y\": ['slim', 'emotional', 'nurturing', 'sensitive', 'graceful'],\n",
    "        \"H3_X\": ['recovery', 'survival', 'effective', 'promising', 'cure', 'beneficial'],\n",
    "        \"H3_Y\": ['mortality', 'complication', 'effect', 'contraindication', 'adverse', 'risk'],\n",
    "        \"H4_X\": ['std', 'hiv','gonorrhea'],\n",
    "        \"H4_Y\": ['herpes', 'chlamydia','syphilis']\n",
    "    }\n",
    "    \n",
    "\n",
    "# Instantiate the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the articles\n",
    "tfidf_matrix = vectorizer.fit_transform(articles)\n",
    "\n",
    "# Extract word frequencies using sum of TF-IDF scores across all documents\n",
    "word_freq = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).tolist()[0]))\n",
    "\n",
    "def get_word_frequency(word):\n",
    "    return word_freq.get(word, 0)\n",
    "\n",
    "def trim_list_by_least_frequent(larger_list, size_to_trim):\n",
    "    # Sort list by frequency\n",
    "    sorted_list = sorted(larger_list, key=get_word_frequency)\n",
    "    return sorted_list[:size_to_trim]\n",
    "\n",
    "for key, value in list(attribute_sets.items()):\n",
    "    pair_key = key[:-2]  # Gets \"B1\", \"B2\", ...\n",
    "    X_key = pair_key + \"_X\"\n",
    "    Y_key = pair_key + \"_Y\"\n",
    "    \n",
    "    X_list = attribute_sets[X_key]\n",
    "    Y_list = attribute_sets[Y_key]\n",
    "    \n",
    "    if len(X_list) != len(Y_list):\n",
    "        if len(X_list) < len(Y_list):\n",
    "            attribute_sets[Y_key] = trim_list_by_least_frequent(Y_list, len(X_list))\n",
    "        else:\n",
    "            attribute_sets[X_key] = trim_list_by_least_frequent(X_list, len(Y_list))\n",
    "\n",
    "print(attribute_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85a9df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "def association_score(word, attribute_A, attribute_B, model):\n",
    "    \"\"\"\n",
    "    Compute the association of a word with two sets of attributes.\n",
    "    \"\"\"\n",
    "    if word not in model.wv:\n",
    "        return 0.0  # Return 0 if the word is not in the model's vocabulary\n",
    "\n",
    "    # Filter out missing attributes from A and B\n",
    "    valid_attributes_A = [a for a in attribute_A if a in model.wv]\n",
    "    valid_attributes_B = [b for b in attribute_B if b in model.wv]\n",
    "\n",
    "    if not valid_attributes_A or not valid_attributes_B:\n",
    "        # Return 0 or handle in another way if there are no valid attributes\n",
    "        return 0.0\n",
    "\n",
    "    cosine_similarity_A = np.mean([1 - cosine(model.wv[word], model.wv[a]) for a in valid_attributes_A])\n",
    "    cosine_similarity_B = np.mean([1 - cosine(model.wv[word], model.wv[b]) for b in valid_attributes_B])\n",
    "\n",
    "    return cosine_similarity_A - cosine_similarity_B\n",
    "\n",
    "def weat_score(target_A, target_B, attribute_X, attribute_Y, model):\n",
    "    \"\"\"Calculate the WEAT test statistic s(X, Y, A, B).\"\"\"\n",
    "    score_A = np.sum([association_score(word, attribute_X, attribute_Y, model) for word in target_A])\n",
    "    score_B = np.sum([association_score(word, attribute_X, attribute_Y, model) for word in target_B])\n",
    "    return score_A - score_B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def effect_size(target_A, target_B, attribute_X, attribute_Y, model):\n",
    "    \"\"\"Compute the effect size for the WEAT test.\"\"\"\n",
    "    scores_A = [association_score(word, attribute_X, attribute_Y, model) for word in target_A]\n",
    "    scores_B = [association_score(word, attribute_X, attribute_Y, model) for word in target_B]\n",
    "    scores_all = scores_A + scores_B\n",
    "    \n",
    "    mean_diff = np.mean(scores_A) - np.mean(scores_B)\n",
    "    std_dev = np.std(scores_all)\n",
    "    \n",
    "    return mean_diff / std_dev\n",
    "\n",
    "\n",
    "# Your permutation_test function remains the same. \n",
    "def permutation_test(target_A, target_B, attribute_X, attribute_Y, model, n_permutations=1000):\n",
    "    \n",
    "    # Calculate the observed test statistic\n",
    "    observed_score = weat_score(target_A, target_B, attribute_X, attribute_Y, model)\n",
    "    \n",
    "    # Pool all the words\n",
    "    pooled_words = target_A + target_B\n",
    "    \n",
    "    extreme_count = 0\n",
    "    for _ in range(n_permutations):\n",
    "        # Randomly assign words to two groups\n",
    "        random.shuffle(pooled_words)\n",
    "        perm_target_A = pooled_words[:len(target_A)]\n",
    "        perm_target_B = pooled_words[len(target_A):]\n",
    "        \n",
    "        # Calculate the test statistic for the permuted groups\n",
    "        perm_score = weat_score(perm_target_A, perm_target_B, attribute_X, attribute_Y, model)\n",
    "        \n",
    "        # Check if the permuted test statistic is as extreme as the observed test statistic\n",
    "        if abs(perm_score) >= abs(observed_score):\n",
    "            extreme_count += 1\n",
    "            \n",
    "    # The p-value is the proportion of permuted test statistics as extreme as the observed test statistic\n",
    "    p_value = extreme_count / n_permutations\n",
    "    \n",
    "    return observed_score, p_value\n",
    "\n",
    "missing_words_1 = []\n",
    "missing_words_2 = []\n",
    "def run_weat_tests_2(model1, model2, n_permutations=1000):\n",
    "    # Define target and attribute sets\n",
    "    target_sets = {\n",
    "        \"M\": ['male', 'man', 'boy', 'brother', 'son', 'father', 'uncle', 'grandfather'],\n",
    "        \"F\": ['female', 'woman', 'girl', 'sister','daughter', 'mother', 'aunt', 'grandmother']\n",
    "    }\n",
    "    \n",
    "    attribute_sets = {'B1_X': ['salary', 'corporation', 'business', 'executive', 'office', 'career'], 'B1_Y': ['home', 'parents', 'children', 'family', 'marriage', 'wedding'], 'B2_X': ['math', 'geometry', 'calculus', 'computation', 'addition'], 'B2_Y': ['symphony', 'poetry', 'shakespeare', 'drama', 'dance'], 'B3_X': ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment'], 'B3_Y': ['symphony', 'poetry', 'shakespeare', 'drama', 'dance', 'novel', 'literature'], 'B4_X': ['precocious', 'resourceful', 'genius', 'inventive', 'astute', 'adaptable', 'reflective', 'intuitive', 'judicious', 'analytical', 'apt', 'venerable', 'imaginative', 'shrewd', 'wise', 'smart', 'clever', 'brilliant', 'logical', 'intelligent'], 'B4_Y': ['alluring', 'feeble', 'voluptuous', 'stout', 'plump', 'ugly', 'gorgeous', 'fashionable', 'slender', 'sensual', 'handsome', 'pretty', 'bald', 'slim', 'attractive', 'beautiful', 'athletic', 'muscular', 'thin', 'weak'], 'B5_X': ['confident', 'triumph', 'bold', 'loud', 'shout', 'command', 'succeed', 'dynamic', 'assert', 'winner', 'dominant', 'leader'], 'B5_Y': ['weak', 'surrender', 'vulnerable', 'weakness', 'withdraw', 'yield', 'failure', 'shy', 'follow', 'lose', 'fragile', 'afraid'], 'H1_X': ['surgeon', 'doctor', 'orthopedic', 'paramedic', 'pharmacist'], 'H1_Y': ['nurse', 'midwife', 'pediatrician', 'assistant', 'dietician'], 'H2_X': ['muscular', 'athletic', 'stoic', 'strong', 'unemotional'], 'H2_Y': ['slim', 'emotional', 'nurturing', 'sensitive', 'graceful'], 'H3_X': ['recovery', 'survival', 'effective', 'promising', 'cure', 'beneficial'], 'H3_Y': ['mortality', 'complication', 'effect', 'contraindication', 'adverse', 'risk'], 'H4_X': ['std', 'hiv', 'gonorrhea'], 'H4_Y': ['herpes', 'chlamydia', 'syphilis']}\n",
    "    \n",
    "    # Check for missing words in Model 1\n",
    "    \n",
    "    for key, words in target_sets.items():\n",
    "        missing_words_1.extend([word for word in words if word not in model1.wv])\n",
    "    for key, words in attribute_sets.items():\n",
    "        missing_words_1.extend([word for word in words if word not in model1.wv])\n",
    "\n",
    "    if missing_words_1:\n",
    "        print(\"Model1 Missing words:\", set(missing_words_1))  # Using set to remove duplicates\n",
    "\n",
    "    # Check for missing words in Model 2\n",
    "    \n",
    "    for key, words in target_sets.items():\n",
    "        missing_words_2.extend([word for word in words if word not in model2.wv])\n",
    "    for key, words in attribute_sets.items():\n",
    "        missing_words_2.extend([word for word in words if word not in model2.wv])\n",
    "\n",
    "    if missing_words_2:\n",
    "        print(\"\\nModel2 Missing words:\", set(missing_words_2))  # Using set to remove duplicates\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Iterate through the attribute sets and compute the WEAT score, effect size, and p-value for both models\n",
    "    for key in attribute_sets:\n",
    "        if \"_X\" in key:\n",
    "            bias_name = key.split(\"_\")[0]\n",
    "            \n",
    "            # For model1\n",
    "            score1, p_value1 = permutation_test(target_sets[\"M\"], target_sets[\"F\"], attribute_sets[bias_name+\"_X\"], attribute_sets[bias_name+\"_Y\"], model1, n_permutations)\n",
    "            effect_size1 = effect_size(target_sets[\"M\"], target_sets[\"F\"], attribute_sets[bias_name+\"_X\"], attribute_sets[bias_name+\"_Y\"], model1)\n",
    "            \n",
    "            # For model2\n",
    "            score2, p_value2 = permutation_test(target_sets[\"M\"], target_sets[\"F\"], attribute_sets[bias_name+\"_X\"], attribute_sets[bias_name+\"_Y\"], model2, n_permutations)\n",
    "            effect_size2 = effect_size(target_sets[\"M\"], target_sets[\"F\"], attribute_sets[bias_name+\"_X\"], attribute_sets[bias_name+\"_Y\"], model2)\n",
    "            \n",
    "            \n",
    "            results.append((bias_name, score1, effect_size1, p_value1, score2, effect_size2, p_value2))\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame(results, columns=[\"Bias\", \"WEAT Score (Model 1)\", \"Effect Size (Model 1)\", \"p-value (Model 1)\", \"WEAT Score (Model 2)\", \"Effect Size (Model 2)\", \"p-value (Model 2)\"])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d13c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run sweat.py\n",
    "\n",
    "from sweat import SWEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7af0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "men_model_1 = Word2Vec.load(\"men_model_1.model\")\n",
    "women_model = Word2Vec.load(\"women_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29398c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweat_test(men_model,women_model):\n",
    "    target_sets = {\n",
    "        \"M\": ['male', 'man', 'boy', 'brother', 'son', 'father', 'uncle', 'grandfather'],\n",
    "        \"F\": ['female', 'woman', 'girl', 'sister','daughter', 'mother', 'aunt', 'grandmother']\n",
    "        }\n",
    "    swt = SWEAT(men_model, women_model, target_sets['M'], target_sets['F'])\n",
    "    \n",
    "    attribute_sets = {'B1_X': ['salary', 'corporation', 'business', 'executive', 'office', 'career'], 'B1_Y': ['home', 'children', 'family', 'marriage', 'wedding'], 'B2_X': ['math', 'geometry', 'calculus', 'computation', 'addition'], 'B2_Y': ['symphony', 'poetry', 'shakespeare', 'drama', 'dance'], 'B3_X': ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment'], 'B3_Y': ['symphony', 'poetry', 'shakespeare', 'drama', 'dance', 'novel', 'literature'], 'B4_X': ['precocious', 'resourceful', 'genius', 'inventive', 'astute', 'adaptable', 'reflective', 'intuitive', 'judicious', 'analytical', 'apt', 'venerable', 'imaginative', 'shrewd', 'wise', 'smart', 'clever', 'brilliant', 'logical', 'intelligent'], 'B4_Y': ['alluring', 'feeble', 'voluptuous', 'stout', 'plump', 'ugly', 'gorgeous', 'fashionable', 'slender', 'sensual', 'handsome', 'pretty', 'bald', 'slim', 'attractive', 'beautiful', 'athletic', 'muscular', 'thin', 'weak'], 'B5_X': ['confident', 'triumph', 'bold', 'loud', 'shout', 'command', 'succeed', 'dynamic', 'assert', 'winner', 'dominant', 'leader'], 'B5_Y': ['weak', 'surrender', 'vulnerable', 'weakness', 'withdraw', 'yield', 'failure', 'shy', 'follow', 'lose', 'fragile', 'afraid'], 'H1_X': ['surgeon', 'doctor', 'orthopedic', 'paramedic', 'pharmacist'], 'H1_Y': ['nurse', 'midwife', 'pediatrician', 'assistant', 'dietician'], 'H2_X': ['muscular', 'athletic', 'stoic', 'strong', 'unemotional'], 'H2_Y': ['slim', 'emotional', 'nurturing', 'sensitive', 'graceful'], 'H3_X': ['recovery', 'survival', 'effective', 'promising', 'cure', 'beneficial'], 'H3_Y': ['mortality', 'complication', 'effect', 'contraindication', 'adverse', 'risk'], 'H4_X': ['std', 'hiv', 'gonorrhea'], 'H4_Y': ['herpes', 'chlamydia', 'syphilis']}\n",
    "    results = []\n",
    "    for key in attribute_sets:\n",
    "        attributes = attribute_sets[key]\n",
    "        valid_attributes_A = [a for a in attributes if a in men_model.wv]\n",
    "        valid_attributes_B = [b for b in attributes if b in women_model.wv]\n",
    "        valid_attributes = list(set(valid_attributes_A) & set(valid_attributes_B))\n",
    "        sweat_result = swt.test(valid_attributes)\n",
    "        results.append((key, sweat_result['score'], sweat_result['eff_size'], sweat_result['p-val']))\n",
    "    df = pd.DataFrame(results, columns=[\"Bias\", \"SWEAT Score\", \"Effect Size\", \"p-value\"])\n",
    "    return df\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "913bce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweat_results = run_sweat_test(men_model_1,women_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f1f9ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bias</th>\n",
       "      <th>SWEAT Score</th>\n",
       "      <th>Effect Size</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1_X</td>\n",
       "      <td>-0.1244</td>\n",
       "      <td>-1.3748</td>\n",
       "      <td>0.0275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1_Y</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.3270</td>\n",
       "      <td>0.6543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B2_X</td>\n",
       "      <td>-0.0084</td>\n",
       "      <td>-0.1082</td>\n",
       "      <td>0.9061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B2_Y</td>\n",
       "      <td>-0.3320</td>\n",
       "      <td>-3.9265</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B3_X</td>\n",
       "      <td>-0.2219</td>\n",
       "      <td>-1.3025</td>\n",
       "      <td>0.1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B3_Y</td>\n",
       "      <td>-0.4012</td>\n",
       "      <td>-2.2679</td>\n",
       "      <td>0.0055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B4_X</td>\n",
       "      <td>-0.8425</td>\n",
       "      <td>-3.4855</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B4_Y</td>\n",
       "      <td>-1.3068</td>\n",
       "      <td>-2.0310</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B5_X</td>\n",
       "      <td>-0.6641</td>\n",
       "      <td>-3.6948</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B5_Y</td>\n",
       "      <td>-0.4679</td>\n",
       "      <td>-1.2780</td>\n",
       "      <td>0.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>H1_X</td>\n",
       "      <td>-0.2137</td>\n",
       "      <td>-1.4095</td>\n",
       "      <td>0.0741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>H1_Y</td>\n",
       "      <td>-0.0927</td>\n",
       "      <td>-1.1957</td>\n",
       "      <td>0.2316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>H2_X</td>\n",
       "      <td>-0.2527</td>\n",
       "      <td>-1.4249</td>\n",
       "      <td>0.1140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H2_Y</td>\n",
       "      <td>-0.0991</td>\n",
       "      <td>-0.6219</td>\n",
       "      <td>0.4975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>H3_X</td>\n",
       "      <td>-0.0638</td>\n",
       "      <td>-0.3814</td>\n",
       "      <td>0.5659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H3_Y</td>\n",
       "      <td>0.0628</td>\n",
       "      <td>0.4965</td>\n",
       "      <td>0.4431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H4_X</td>\n",
       "      <td>-0.0331</td>\n",
       "      <td>-0.2634</td>\n",
       "      <td>0.7018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>H4_Y</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>-66.4024</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Bias  SWEAT Score  Effect Size  p-value\n",
       "0   B1_X      -0.1244      -1.3748   0.0275\n",
       "1   B1_Y       0.0573       0.3270   0.6543\n",
       "2   B2_X      -0.0084      -0.1082   0.9061\n",
       "3   B2_Y      -0.3320      -3.9265   0.0000\n",
       "4   B3_X      -0.2219      -1.3025   0.1195\n",
       "5   B3_Y      -0.4012      -2.2679   0.0055\n",
       "6   B4_X      -0.8425      -3.4855   0.0000\n",
       "7   B4_Y      -1.3068      -2.0310   0.0000\n",
       "8   B5_X      -0.6641      -3.6948   0.0000\n",
       "9   B5_Y      -0.4679      -1.2780   0.0063\n",
       "10  H1_X      -0.2137      -1.4095   0.0741\n",
       "11  H1_Y      -0.0927      -1.1957   0.2316\n",
       "12  H2_X      -0.2527      -1.4249   0.1140\n",
       "13  H2_Y      -0.0991      -0.6219   0.4975\n",
       "14  H3_X      -0.0638      -0.3814   0.5659\n",
       "15  H3_Y       0.0628       0.4965   0.4431\n",
       "16  H4_X      -0.0331      -0.2634   0.7018\n",
       "17  H4_Y      -0.1564     -66.4024   0.0000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweat_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "732858ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cumulative visualizations\n",
    "swt.plot_cumulative(attribute_sets['B1_X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64564717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x345.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cumulative visualizations\n",
    "swt.plot_details(attribute_sets['B1_X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4427e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
